{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://deeplearning4j.org/model-import-keras\n",
    "# https://github.com/amir-abdi/Deep-Learning-Papers-Reading-Roadmap\n",
    "# http://www.deeplearningbook.org/\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split #pip install -U scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pip install --upgrade keras\n",
    "#pip install --upgrade tensorflow\n",
    "#..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA:  (1539, 10)\n",
      "TEST  DATA:  (660, 10)\n"
     ]
    }
   ],
   "source": [
    "#Loading the data\n",
    "datos = pd.read_csv('datos.csv')\n",
    "\n",
    "#Shuffle\n",
    "#np.random.shuffle(datos)\n",
    "#datos = datos.sample(frac=1).reset_index(drop=True)\n",
    "datos.sample(frac=1)\n",
    "\n",
    "x_data = datos.drop(labels=['PTO_X', 'PTO_Y'], axis=1)\n",
    "y_data = datos.drop(['WI0', 'WI1', 'WI2', 'WI3', 'WI4', 'WI5', 'WI6', 'WI7', 'WI8', 'WI9'], axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=101)\n",
    "\n",
    "print(\"TRAIN DATA: \", x_train.shape)\n",
    "print(\"TEST  DATA: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "\n",
    "\"\"\"\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "scaler.fit(x_test)\n",
    "\"\"\"\n",
    "#OR\n",
    "\"\"\"\n",
    "mean = x_train.mean(axis=0)\n",
    "x_train -= mean\n",
    "std = x_train.std(axis=0)\n",
    "x_train /= std\n",
    "x_test -= mean\n",
    "x_test /= std\n",
    "#--> Despreciable, si no lo haces en x_test deber√≠as ver que no cambia error\n",
    "print(\"MEAN:\\n\",mean, \"\\nSTD DEV:\\n\",std)\n",
    "\"\"\"\n",
    "#OR\n",
    "\"\"\"\n",
    "min = x_train.min().min()\n",
    "max = x_train.max().max()\n",
    "print(\"MIN: %f  MAX: %f\" % (min, max))\n",
    "x_train = (x_train - min)/(max - min)\n",
    "x_test = (x_test - min)/(max - min)\n",
    "\"\"\"\n",
    "#OR\n",
    "mean = x_train.mean(axis=0).mean()\n",
    "std  = x_train.std(axis=0).std()\n",
    "print(\"MEAN: %f,  STD DEV: %f\" % (mean, std))\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = (x_test - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#separar validate & test data ?\n",
    "x_valid = x_train[:200]\n",
    "x_train2 = x_train[200:]\n",
    "y_valid = y_train[:200]\n",
    "y_train2 = y_train[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ANN Model\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(7, activation='relu', input_shape=(x_data.shape[1],), name='cesin'))\n",
    "    model.add(layers.Dense(7, activation='relu'))\n",
    "    model.add(layers.Dense(7, activation='relu'))\n",
    "    model.add(layers.Dense(2, name='cesout'))\n",
    "    model.compile(optimizer=optimizers.RMSprop(lr=0.00005), loss=losses.mse, metrics=[metrics.mae])\n",
    "    #model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train the model  (ep: 22000)\n",
    "model = build_model()\n",
    "#history = model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=100, verbose=0)\n",
    "hist = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=22000, batch_size=100, verbose=0)\n",
    "#history = model.fit(x_train2, y_train2, validation_data=(x_valid, y_valid), epochs=15000, batch_size=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-62a850923c77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#Study the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Min error %f at epoch %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist' is not defined"
     ]
    }
   ],
   "source": [
    "#Study the model\n",
    "\n",
    "print(\"Min error %f at epoch %d\" % (np.min(hist.history['val_loss'][:]), np.argmin(hist.history['val_loss'][:])))\n",
    "\n",
    "loss = hist.history['loss'][2000:]\n",
    "val_loss = hist.history['val_loss'][2000:]\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_mse_score, test_mae_score = model.evaluate(x_test, y_test)\n",
    "test_mae_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- <class 'pandas.core.frame.DataFrame'>  --  <class 'numpy.ndarray'>\n",
      "(660, 2)\n",
      "\n",
      "\n",
      "CES ERROR:  17.329334148597265\n",
      "\n",
      "\n",
      "---------------> Real vs Pred.: (69, 29) vs (54, 40)\n",
      "---------------> Real vs Pred.: (69, 31) vs (57, 43)\n",
      "---------------> Real vs Pred.: (77, 87) vs (83, 73)\n",
      "---------------> Real vs Pred.: (38, 87) vs (70, 58)\n",
      "---------------> Real vs Pred.: (85, 77) vs (88, 79)\n",
      "---------------> Real vs Pred.: (73, 51) vs (66, 53)\n",
      "---------------> Real vs Pred.: (65, 55) vs (62, 48)\n",
      "---------------> Real vs Pred.: (46, 84) vs (77, 65)\n",
      "---------------> Real vs Pred.: (78, 68) vs (84, 74)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CES error\n",
    "y_predicted = model.predict(x_test)\n",
    "y_test0 = np.asarray(y_test)\n",
    "print(\"-------\", type(y_test), \" -- \", type(y_test0))\n",
    "print(y_predicted.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "#\n",
    "def calcError(real, pred):\n",
    "    error = 0.\n",
    "    real0 = np.asarray(real)\n",
    "    for i in range(real.shape[0]):\n",
    "        error = error + np.sqrt(pow(pred[i][0] - real0[i][0], 2) + pow(pred[i][1] - real0[i][1], 2))\n",
    "    return error  /  real.shape[0]\n",
    "print(\"CES ERROR: \", calcError(y_test0, y_predicted))\n",
    "print(\"\\n\")\n",
    "\n",
    "#\n",
    "for i in range(1,10):\n",
    "    print(\"---------------> Real vs Pred.: (%.0f, %.0f) vs (%.0f, %.0f)\"\n",
    "          % (y_test0[i][0], y_test0[i][1], y_predicted[i][0], y_predicted[i][1]))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save the Keras model\n",
    "model.save('cesnet.h5')# creates a HDF5 file 'my_model.h5'\n",
    "#del model  # deletes the existing model\n",
    "from keras.models import load_model\n",
    "model2 = load_model('cesnet.h5')\n",
    "model3 = load_model('cesnet - error7.6.h5')\n",
    "\n",
    "#\n",
    "y_predicted = model2.predict(x_test)\n",
    "del model2\n",
    "print(\"CES ERROR: \", calcError(y_test, y_predicted))\n",
    "#\n",
    "y_predicted = model3.predict(x_test)\n",
    "del model3\n",
    "print(\"CES ERROR: \", calcError(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save the Protobuf (Tensorflow) model\n",
    "#python keras_to_tf.py -input_model_file cesnet.h5 -output_model_file cesnet.pb\n",
    "%run keras_to_tf.py -input_model_file cesnet.h5 -output_model_file cesnet.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the Protobuf (Tensorflow) model\n",
    "#https://stackoverflow.com/questions/44274701/make-predictions-using-a-tensorflow-graph-from-a-keras-model\n",
    "def load_graph(frozen_graph_filename):\n",
    "    # We load the protobuf file from the disk and parse it to retrieve the unserialized graph_def\n",
    "    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    # Then, we can use again a convenient built-in function to import a graph_def into the current default Graph\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(\n",
    "            graph_def, \n",
    "            input_map=None, \n",
    "            return_elements=None, \n",
    "            name=\"prefix\", \n",
    "            op_dict=None, \n",
    "            producer_op_list=None\n",
    "        )\n",
    "\n",
    "    input_name = graph.get_operations()[0].name+':0'     #prefix/dense_1_input:0\n",
    "    output_name = graph.get_operations()[-1].name+':0'   #prefix/output_node0:0\n",
    "\n",
    "    return graph, input_name, output_name\n",
    "\n",
    "#g, i, o = load_graph('cesnet.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run the Protobuf (Tensorflow) model\n",
    "def predict(model_path, input_data):\n",
    "    # load tf graph\n",
    "    tf_model, tf_input, tf_output = load_graph(model_path)\n",
    "\n",
    "    # Create tensors for model input and output\n",
    "    x = tf_model.get_tensor_by_name(tf_input)\n",
    "    y = tf_model.get_tensor_by_name(tf_output) \n",
    "\n",
    "    # Number of model outputs\n",
    "    num_outputs = y.shape.as_list()[1]\n",
    "    print(y.shape)\n",
    "    print(num_outputs)\n",
    "    print(input_data.shape[0])\n",
    "    predictions = np.zeros((input_data.shape[0], num_outputs))\n",
    "    for i in range(input_data.shape[0]):        \n",
    "        with tf.Session(graph=tf_model) as sess:\n",
    "            y_out = sess.run(y, feed_dict={x: input_data[i:i+1]})\n",
    "            predictions[i] = y_out\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_predictions = predict('cesnet.pb', x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"CES ERROR: \", calcError(y_test, tf_predictions))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO: Importar en Android\n",
    "https://deeplearning4j.org/model-import-keras\n",
    "\n",
    "HDF5 isn't supported on Android, so import on the desktop, save, and load on Android.\n",
    "net = KerasModelImport.importKerasSequentialModelAndWeights( dir + \"cesnet.h5\" )\n",
    "\n",
    "Problem arises if use Keras-2. So, tried to save model using Keras-1.1.0 and it works perfectly.\n",
    "Also able to import the model on android.\n",
    "\n",
    "$ pip install keras==1.2\n",
    "$ python -c \"import keras; print(keras.__version__)\"\n",
    "\n",
    "Tampoco vale pasar a keras 1, porque cambiar√≠a la estructura de mi red,\n",
    "mejor esperar a DL4J 0.92 que si permite la importaci√≥n desde keras 2\n",
    "(o compilar el actual snapshot que tambien soporta keras 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras; print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
